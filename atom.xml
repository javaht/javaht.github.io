<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://javaht.github.io</id>
    <title>周记</title>
    <updated>2022-05-04T15:07:40.736Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="http://javaht.github.io"/>
    <link rel="self" href="http://javaht.github.io/atom.xml"/>
    <subtitle>不知名周某人之记录</subtitle>
    <logo>http://javaht.github.io/images/avatar.png</logo>
    <icon>http://javaht.github.io/favicon.ico</icon>
    <rights>All rights reserved 2022, 周记</rights>
    <entry>
        <title type="html"><![CDATA[Spark的优化]]></title>
        <id>http://javaht.github.io/post/spark-de-you-hua/</id>
        <link href="http://javaht.github.io/post/spark-de-you-hua/">
        </link>
        <updated>2022-05-04T14:30:07.000Z</updated>
        <content type="html"><![CDATA[<h1 id="spark做过哪些优化为什么原理是什么">Spark做过哪些优化（为什么？原理是什么？）</h1>
<h3 id="1首先是资源调优">1.首先是资源调优</h3>
<p>从提交参数：<br>
1.1 executor-cores：每个excutor的最大核数 一般是3到6<br>
1.2 num-executors：其实是总的excutor数量  就是单个节点的executor数量*节点数。<br>
(每个节点的executor数量 =单节点yarn的总核数/单个excutor的核数)<br>
总的excutor数量 = (单节点yarn的总核数/单个excutor的核数)*节点数<br>
1.3 executor-memory：单个executor的内存=yarn上配置的单个节点内存/excutor数量</p>
<p>RDD持久化： rdd持久化可以将数据复用<br>
cache() 和persist()   其实cache就是默认的persist() 级别是disk和memory</p>
<h3 id="2cpu优化">2.CPU优化</h3>
<p>RDD的并行度：一个job一次所能执行的task数目，即一个job对应的总的core资源个数(<strong>spark.default.parallelism</strong>)<br>
(没有设置时，由 join、reduceByKey 和 parallelize 等转换决定。)<br>
SparkSql的并行度：默认是200 只能控制Spark sql、DataFrame、DataSet 分区个数<br>
不能控制 RDD 分区个数  <strong>spark.sql.shuffle.partitions</strong></p>
<p>并发度：同时执行的 task 数 有几个core并发度就是多少</p>
<p>一般设置并行度是并发度(就是有多少个core)的2到三倍</p>
<p>CPU 低效原因<br>
1）并行度较低、数据分片较大容易导致 CPU 线程挂起<br>
2）并行度过高、数据过于分散会让调度开销更多</p>
<h3 id="3sparksql语法优化">3.SparkSql语法优化</h3>
<p>3.1基于RBO的优化(基于规则的优化器)   主要分为三类 1.谓词下推 2.列裁剪 3.常量替换<br>
3.2基于RBO的优化(基于代价的优化器)<br>
生成表级别统计信息（扫表）：ANALYZE TABLE 表名 COMPUTE STATISTICS<br>
生成表级别统计信息（不扫表）：ANALYZE TABLE src COMPUTE STATISTICS NOSCAN<br>
生成列级别统计信息:ANALYZE TABLE 表名 COMPUTE STATISTICS FOR COLUMNS 列 1,列 2,列 3<br>
DESC FORMATTED 表名</p>
<p>3.3广播join<br>
通过参数指定自动广播  .set(&quot;spark.sql.autoBroadcastJoinThreshold&quot;,&quot;10m&quot;)<br>
强行广播： select  /*+  BROADCASTJOIN(sc) */   sc是表的别名</p>
<p>3.4 SMB join sort merge bucket 操作,首先会进行排序,然后根据 key值合并，把相同 key 的数据放到同一个 bucket 中（按照 key 进行 hash）。<br>
使用条件：1.两表进行分桶，桶的个数必须相等  2.两边进行 join 时，join的列=排序的列=分桶的列</p>
<h3 id="4数据倾斜">4.数据倾斜</h3>
<h4 id="41产生原因">4.1产生原因？</h4>
<p>设计shuffle类的算子时，某个key的数量特别大</p>
<h4 id="42-数据倾斜大-key-定位">4.2 数据倾斜大 key 定位</h4>
<p>使用sample算子对key不放回采样 计算出现的次数后排序取前十</p>
<h4 id="43-单表数据倾斜优化">4.3 单表数据倾斜优化 (*)</h4>
<p>两阶段聚合（加盐局部聚合+去盐全局聚合）</p>
<h4 id="44-join数据倾斜优化">4.4 join数据倾斜优化</h4>
<h5 id="441-广播-join">4.4.1 广播 Join</h5>
<p>适用于小表 join 大表。小表足够小，可被加载进 Driver 并通过 Broadcast 方法广播到各个 Executor 中</p>
<h5 id="442-拆分大-key-打散大表-扩容小表">4.4.2 拆分大 key 打散大表 扩容小表</h5>
<p>​	1.首先将大表分为带有倾斜key和不带有倾斜key的</p>
<pre><code>   1. 将倾斜的key打散  打散36份
   2. 小表进行扩容 扩大36倍
   3. 倾斜的大key和扩容后的表进行join
   4. 没有倾斜大key的部分 与 原来的表进行join
   5. 将倾斜key join后的结果与普通key join后的结果 union起来
</code></pre>
<h3 id="5job优化">5.job优化</h3>
<h4 id="51map端聚合">5.1Map端聚合</h4>
<p>RDD 的话建议使用 reduceByKey 或者 aggregateByKey 算子来替代掉 groupByKey 算子。因为 reduceByKey 和 aggregateByKey 算子都会使用用户自定义的函数对每个节点本地的相同 key 进行预聚合。而 groupByKey 算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。<br>
SparkSQL 本身的 HashAggregte 就会实现本地预聚合+全局聚合</p>
<h4 id="52读取小文件优化">5.2读取小文件优化</h4>
<p>spark.sql.files.maxPartitionBytes=128MB  (设置每个分区最大为128M  默认 128m)<br>
spark.files.openCostInBytes=4194304   （打开文件的开销）默认 4m</p>
<p>totalBytes = （单个文件的大小+openCostInbytes）*文件个数</p>
<p>bytesPerCore =totalBytes/defaultParallelism  文件的总大小/默认的并行度<br>
Math.min(defaultMaxSplitBytes,Math.max(openCostInbytes,bytesPerCore))</p>
<p>当（文件 1 大小+ openCostInBytes）+（文件 2 大小+ openCostInBytes）+…+（文件n-1 大小+ openCostInBytes）+ 文件 n &lt;= maxPartitionBytes 时，n 个文件可以读入同一个分区，即满足： N 个小文件总大小 + （N-1）*openCostInBytes &lt;= maxPartitionBytes 的话</p>
<h4 id="53-增大-map-溢写时输出流-buffer">5.3 增大 map 溢写时输出流 buffer</h4>
<figure data-type="image" tabindex="1"><img src="https://s2.loli.net/2022/05/04/8CKke45qThaDMsR.png" alt="image-20220504180522834" loading="lazy"></figure>
<pre><code>  buffer缓冲区大小是5M(超过会*2) 和缓冲的批次10000条 这两个参数无效   
  溢写时使用输出流缓冲区默认 32k  我们可以修改这个 
  .set(&quot;spark.shuffle.file.buffer&quot;, &quot;64&quot;)
</code></pre>
<h4 id="54-reduce端的优化">5.4 Reduce端的优化</h4>
<h5 id="541-合理设置reduce的个数">5.4.1 合理设置reduce的个数</h5>
<p>就是shuffle后的分区数 也就是task的数量 比如sparksql的默认200  并行度是并发度的2到3倍</p>
<h5 id="542输出产生的小文件优化">5.4.2输出产生的小文件优化</h5>
<h6 id="一-join-后的结果插入新表">一、Join 后的结果插入新表</h6>
<p>join 结果插入新表，生成的文件数等于 shuffle 并行度，默认就是 200 份文件插入到hdfs 上。</p>
<p>1.可以在插入表数据前进行缩小分区操作来解决小文件过多问题，如 coalesce、repartition 算子。<br>
2.调整 shuffle 并行度。</p>
<p>RDD的并行度：一个job一次所能执行的task数目，<strong>spark.default.parallelism</strong><br>
SparkSql的并行度：默认是200 只能控制Spark sql、DataFrame、DataSet 分区个数，不能控制 RDD 分区个数  <em><strong>spark.sql.shuffle.partitions</strong></em></p>
<h6 id="二-动态分区插入数据">二、动态分区插入数据</h6>
]]></content>
    </entry>
</feed>