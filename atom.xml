<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://javaht.github.io</id>
    <title>周记</title>
    <updated>2022-05-05T14:20:37.787Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="http://javaht.github.io"/>
    <link rel="self" href="http://javaht.github.io/atom.xml"/>
    <subtitle>不知名周某人之记录</subtitle>
    <logo>http://javaht.github.io/images/avatar.png</logo>
    <icon>http://javaht.github.io/favicon.ico</icon>
    <rights>All rights reserved 2022, 周记</rights>
    <entry>
        <title type="html"><![CDATA[Spark的优化]]></title>
        <id>http://javaht.github.io/post/spark-de-you-hua/</id>
        <link href="http://javaht.github.io/post/spark-de-you-hua/">
        </link>
        <updated>2022-05-04T14:30:07.000Z</updated>
        <content type="html"><![CDATA[<h1 id="spark做过哪些优化为什么原理是什么">Spark做过哪些优化（为什么？原理是什么？）</h1>
<h3 id="1首先是资源调优">1.首先是资源调优</h3>
<p>从提交参数：<br>
1.1 executor-cores：每个excutor的最大核数 一般是3到6<br>
1.2 num-executors：其实是总的excutor数量  就是单个节点的executor数量*节点数。<br>
(每个节点的executor数量 =单节点yarn的总核数/单个excutor的核数)<br>
总的excutor数量 = (单节点yarn的总核数/单个excutor的核数)*节点数<br>
1.3 executor-memory：单个executor的内存=yarn上配置的单个节点内存/excutor数量</p>
<p>RDD持久化： rdd持久化可以将数据复用<br>
cache() 和persist()   其实cache就是默认的persist() 级别是disk和memory</p>
<h3 id="2cpu优化">2.CPU优化</h3>
<p>RDD的并行度：一个job一次所能执行的task数目，即一个job对应的总的core资源个数(<strong>spark.default.parallelism</strong>)<br>
(没有设置时，由 join、reduceByKey 和 parallelize 等转换决定。)<br>
SparkSql的并行度：默认是200 只能控制Spark sql、DataFrame、DataSet 分区个数<br>
不能控制 RDD 分区个数  <strong>spark.sql.shuffle.partitions</strong></p>
<p>并发度：同时执行的 task 数 有几个core并发度就是多少</p>
<p>一般设置并行度是并发度(就是有多少个core)的2到三倍</p>
<p>CPU 低效原因<br>
1）并行度较低、数据分片较大容易导致 CPU 线程挂起<br>
2）并行度过高、数据过于分散会让调度开销更多</p>
<h3 id="3sparksql语法优化">3.SparkSql语法优化</h3>
<p>3.1基于RBO的优化(基于规则的优化器)   主要分为三类 1.谓词下推 2.列裁剪 3.常量替换<br>
3.2基于RBO的优化(基于代价的优化器)<br>
生成表级别统计信息（扫表）：ANALYZE TABLE 表名 COMPUTE STATISTICS<br>
生成表级别统计信息（不扫表）：ANALYZE TABLE src COMPUTE STATISTICS NOSCAN<br>
生成列级别统计信息:ANALYZE TABLE 表名 COMPUTE STATISTICS FOR COLUMNS 列 1,列 2,列 3<br>
DESC FORMATTED 表名</p>
<p>3.3广播join<br>
通过参数指定自动广播  .set(&quot;spark.sql.autoBroadcastJoinThreshold&quot;,&quot;10m&quot;)<br>
强行广播： select  /*+  BROADCASTJOIN(sc) */   sc是表的别名</p>
<p>3.4 SMB join sort merge bucket 操作,首先会进行排序,然后根据 key值合并，把相同 key 的数据放到同一个 bucket 中（按照 key 进行 hash）。<br>
使用条件：1.两表进行分桶，桶的个数必须相等  2.两边进行 join 时，join的列=排序的列=分桶的列</p>
<h3 id="4数据倾斜">4.数据倾斜</h3>
<h4 id="41产生原因">4.1产生原因？</h4>
<p>设计shuffle类的算子时，某个key的数量特别大</p>
<h4 id="42-数据倾斜大-key-定位">4.2 数据倾斜大 key 定位</h4>
<p>使用sample算子对key不放回采样 计算出现的次数后排序取前十</p>
<h4 id="43-单表数据倾斜优化">4.3 单表数据倾斜优化 (*)</h4>
<p>两阶段聚合（加盐局部聚合+去盐全局聚合）</p>
<h4 id="44-join数据倾斜优化">4.4 join数据倾斜优化</h4>
<h5 id="441-广播-join">4.4.1 广播 Join</h5>
<p>适用于小表 join 大表。小表足够小，可被加载进 Driver 并通过 Broadcast 方法广播到各个 Executor 中</p>
<h5 id="442-拆分大-key-打散大表-扩容小表">4.4.2 拆分大 key 打散大表 扩容小表</h5>
<p>​	1.首先将大表分为带有倾斜key和不带有倾斜key的</p>
<pre><code>   1. 将倾斜的key打散  打散36份
   2. 小表进行扩容 扩大36倍
   3. 倾斜的大key和扩容后的表进行join
   4. 没有倾斜大key的部分 与 原来的表进行join
   5. 将倾斜key join后的结果与普通key join后的结果 union起来
</code></pre>
<h3 id="5job优化">5.job优化</h3>
<h4 id="51map端聚合">5.1Map端聚合</h4>
<p>RDD 的话建议使用 reduceByKey 或者 aggregateByKey 算子来替代掉 groupByKey 算子。因为 reduceByKey 和 aggregateByKey 算子都会使用用户自定义的函数对每个节点本地的相同 key 进行预聚合。而 groupByKey 算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。<br>
SparkSQL 本身的 HashAggregte 就会实现本地预聚合+全局聚合</p>
<h4 id="52读取小文件优化">5.2读取小文件优化</h4>
<p>spark.sql.files.maxPartitionBytes=128MB  (设置每个分区最大为128M  默认 128m)<br>
spark.files.openCostInBytes=4194304   （打开文件的开销）默认 4m</p>
<p>totalBytes = （单个文件的大小+openCostInbytes）*文件个数</p>
<p>bytesPerCore =totalBytes/defaultParallelism  文件的总大小/默认的并行度<br>
Math.min(defaultMaxSplitBytes,Math.max(openCostInbytes,bytesPerCore))</p>
<p>当（文件 1 大小+ openCostInBytes）+（文件 2 大小+ openCostInBytes）+…+（文件n-1 大小+ openCostInBytes）+ 文件 n &lt;= maxPartitionBytes 时，n 个文件可以读入同一个分区，即满足： N 个小文件总大小 + （N-1）*openCostInBytes &lt;= maxPartitionBytes 的话</p>
<h4 id="53-增大-map-溢写时输出流-buffer">5.3 增大 map 溢写时输出流 buffer</h4>
<figure data-type="image" tabindex="1"><img src="https://s2.loli.net/2022/05/04/8CKke45qThaDMsR.png" alt="image-20220504180522834" loading="lazy"></figure>
<pre><code>  buffer缓冲区大小是5M(超过会*2) 和缓冲的批次10000条 这两个参数无效   
  溢写时使用输出流缓冲区默认 32k  我们可以修改这个 
  .set(&quot;spark.shuffle.file.buffer&quot;, &quot;64&quot;)
</code></pre>
<h4 id="54-reduce端的优化">5.4 Reduce端的优化</h4>
<h5 id="541-合理设置reduce的个数">5.4.1 合理设置reduce的个数</h5>
<p>就是shuffle后的分区数 也就是task的数量 比如sparksql的默认200  并行度是并发度的2到3倍</p>
<h5 id="542输出产生的小文件优化">5.4.2输出产生的小文件优化</h5>
<h6 id="一-join-后的结果插入新表">一、Join 后的结果插入新表</h6>
<p>join 结果插入新表，生成的文件数等于 shuffle 并行度，默认就是 200 份文件插入到hdfs 上。</p>
<p>1.可以在插入表数据前进行缩小分区操作来解决小文件过多问题，如 coalesce、repartition 算子。<br>
2.调整 shuffle 并行度。</p>
<p>RDD的并行度：一个job一次所能执行的task数目，<strong>spark.default.parallelism</strong><br>
SparkSql的并行度：默认是200 只能控制Spark sql、DataFrame、DataSet 分区个数，不能控制 RDD 分区个数  <em><strong>spark.sql.shuffle.partitions</strong></em></p>
<h6 id="二-动态分区插入数据">二、动态分区插入数据</h6>
<p>1.没有 Shuffle 的情况下。最差的情况下，每个 Task 中都有表各个分区的记录，那文件数最终文件数将达到 Task 数量 * 表分区数。这种情况下是极易产生小文件的。<br>
INSERT overwrite table A partition ( aa )<br>
SELECT * FROM B;</p>
<p>2.有 Shuffle 的情况下，上面的 Task 数量 就变成了 spark.sql.shuffle.partitions（默认值200）。那么最差情况就会有 spark.sql.shuffle.partitions * 表分区数。当 spark.sql.shuffle.partitions 设 置 过 大 时 ， 小 文 件 问 题 就 产 生 了 ； 当<br>
spark.sql.shuffle.partitions 设置过小时，任务的并行度就下降了，性能随之受到影响。最理想的情况是根据分区字段进行 shuffle，在上面的 sql 中加上 distribute by aa。把同一分区的记录都哈希到同一个分区中去，由一个 Spark 的 Task 进行写入，这样的话只会产生 N 个文件, 但是这种情况下也容易出现数据倾斜的问题。</p>
<p>解决思路：<br>
结合第 4 章解决倾斜的思路，在确定哪个分区键倾斜的情况下，将倾斜的分区键单独<br>
拎出来：<br>
将入库的 SQL 拆成（where 分区 != 倾斜分区键 ）和 （where 分区 = 倾斜分区键） 几<br>
个部分，非倾斜分区键的部分正常 distribute by 分区字段，倾斜分区键的部分 distribute by<br>
随机数，sql 如下：</p>
<p>//1.非倾斜键部分<br>
INSERT overwrite table A partition ( aa )<br>
SELECT *  FROM B where aa != 大 key<br>
distribute by aa;</p>
<p>//2.倾斜键部分<br>
INSERT overwrite table A partition ( aa )<br>
SELECT * FROM B where aa = 大 key<br>
distribute by cast(rand() * 5 as int);</p>
<h5 id="543-增大-reduce-缓冲区减少拉取次数">5.4.3  增大 reduce 缓冲区，减少拉取次数</h5>
<p>Spark Shuffle 过程中，shuffle reduce task 的 buffer 缓冲区大小决定了 reduce task 每次能够缓冲的数据量，也就是每次能够拉取的数据量，如果内存资源较为充足，适当增加拉取数据缓冲区的大小，可以减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。<br>
reduce 端数据拉取缓冲区的大小可以通过 <strong>spark.reducer.maxSizeInFlight</strong> 参数进行设置，默认为 <strong>48MB</strong></p>
<h5 id="544-调节-reduce-端拉取数据重试次数">5.4.4 调节 reduce 端拉取数据重试次数</h5>
<p>reduce 端拉取数据重试次数可以通过 <strong>spark.shuffle.io.maxRetries</strong> 参数进行设置，该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败，<strong>默认为 3</strong></p>
<h5 id="545-调节-reduce-端拉取数据等待间隔">5.4.5 调节 reduce 端拉取数据等待间隔</h5>
<p>reduce 端拉取数据等待间隔可以通过 <strong>spark.shuffle.io.retryWait</strong> 参数进行设置，<strong>默认值为 5s</strong></p>
<h4 id="55-合理利用-bypass">5.5 合理利用 bypass</h4>
<pre><code> .set(&quot;spark.shuffle.sort.bypassMergeThreshold&quot;, &quot;30&quot;) //bypass阈值，默认200,改成30对比效果
</code></pre>
<h3 id="6整体优化">6.整体优化</h3>
<h4 id="61调节数据本地化等待时长">6.1调节数据本地化等待时长</h4>
<p>在 Spark 项目开发阶段，可以使用 client 模式对程序进行测试，此时，可以在本地看到比较全的日志信息，日志信息中有明确的 Task 数据本地化的级别，如果大部分都是<strong>PROCESS_LOCAL(本地)、NODE_LOCAL(节点)</strong>，那么就无需进行调节，但是如果发现很多的级别都是 <strong>RACK_LOCAL(机架)、ANY(不同机架)</strong>，那么需要对本地化的等待时长进行调节，</p>
<pre><code>spark.locality.wait          //建议 6s、10s
spark.locality.wait.process  //建议 60s
spark.locality.wait.node     //建议 30s
spark.locality.wait.rack     //建议 20s
</code></pre>
<h4 id="62-使用堆外内存">6.2 使用堆外内存</h4>
<p><strong>spark.executor.memory</strong>：提交任务时指定的堆内内存。</p>
<p>**spark.executor.memoryOverhead：**堆外内存参数(内存额外开销)。默认开启，默认值为 spark.executor.memory*0.1 并且会与最小值 384mb 做对比，取最大值。</p>
<p>所以 spark on yarn 任务堆内内存申请 1 个 g，而实际去 yarn 申请的内存大于 1 个 g 的原因。</p>
<p><strong>spark.memory.offHeap.size</strong> ： 堆 外 内 存 参 数 ， spark 中 默 认 关 闭 ， 需 要 将spark.memory.enable.offheap.enable 参数设置为 true。</p>
<p>不同版本不同：</p>
<pre><code>spark2.4.5    executor.memory+executor.memoryOverhead+pySparkWorkerMemory
spark3.0.0    executor.memory+memory.offHeap.size(堆外内存)+executor.memoryOverhead(内存开销  默认堆内内存*0.1)+pySparkWorkerMemory
</code></pre>
<h5 id="621使用堆外缓存">6.2.1使用堆外缓存</h5>
<pre><code>// TODO 指定持久化到 堆外内存
result.persist(StorageLevel.OFF_HEAP)
</code></pre>
]]></content>
    </entry>
</feed>